{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaw0k83l4RDb"
      },
      "source": [
        "# Домашнее задание 2\n",
        "\n",
        "В этом задании:\n",
        "\n",
        "1. Сделаем регрессию над данными через scikit-learn: сначала через регресию, потом через бустинг.\n",
        "2. Сравним результаты с константным предсказанием.\n",
        "3. Сделаем нейронную сеть на полносвязных слоях, обучим над теми же данными - и сравним с лин. регрессией и бустингом.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YwY-vclM4RDd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Для воспроизводимости\n",
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fAsMXouy4RDe"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"insurance.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2Z6GtBD4RDe"
      },
      "source": [
        "### Задание №1:\n",
        "Cделайте train/test split на данных в пропорции 0.2/0.8, залейте в лмс код, который в `df_train`, `df_test`\n",
        "сохранит датафрейм с тренировочными и тестовыми данными соответственно.\n",
        "\n",
        "P.S Использовать train_test_split из scikit-learn запрещено - разбивайте вручную через индексы."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1338, 7)"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "idx_test = np.random.choice(df.shape[0], size=int(0.2 * df.shape[0]), replace=False)\n",
        "df_test = df.iloc[idx_test]\n",
        "df_train = df.drop(index=idx_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "sbG88DkD4RDe"
      },
      "outputs": [],
      "source": [
        "df_train = df.iloc[:int(len(df) * 0.8), :]\n",
        "df_test = df.iloc[int(len(df) * 0.8):, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9FZCave4RDe"
      },
      "source": [
        "### Задание №2:\n",
        "Сделайте `OHE` на колонки sex, region, smoker.\n",
        "\n",
        "Нужно сделать как на train, так и на test датасете.\n",
        "Считайте, что исходные датафреймы сохранены в `df_train` и `df_test`.\n",
        "При этом в `df_train`, `df_test` должны остаться старые колонки - т.е. его нужно обогатить.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Создаём OHE и обучаем на train-датасете\n",
        "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')  \n",
        "ohe.fit(df_train[['sex', 'region', 'smoker']])  \n",
        "\n",
        "# Трансформируем train и test\n",
        "df_train_ohe = ohe.transform(df_train[['sex', 'region', 'smoker']])\n",
        "df_test_ohe = ohe.transform(df_test[['sex', 'region', 'smoker']])\n",
        "\n",
        "# Создаём DataFrame с OHE-признаками\n",
        "ohe_columns = ohe.get_feature_names_out(['sex', 'region', 'smoker'])\n",
        "df_train_ohe = pd.DataFrame(df_train_ohe, columns=ohe_columns, index=df_train.index)\n",
        "df_test_ohe = pd.DataFrame(df_test_ohe, columns=ohe_columns, index=df_test.index)\n",
        "\n",
        "# Обогащаем исходные DataFrame\n",
        "df_train = pd.concat([df_train, df_train_ohe], axis=1)\n",
        "df_test = pd.concat([df_test, df_test_ohe], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92K8khao4RDf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky6ESVvn4RDf"
      },
      "source": [
        "### Задание №4:\n",
        "Нормализуйте колонки, которые вы отметили в квизе.\n",
        "Считайте, что исходные датафреймы сохранены в `df_train` и `df_test`.\n",
        "\n",
        "Сдайте код, который модифицирует `df_train` и `df_test` так, чтобы численные колонки из прошлого пункта стали нормированы."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kaGCVzD34RDf"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "StandardScaler()"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scaler.fit(df_train[['age', 'bmi', 'charges']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train[['age', 'bmi', 'charges']] = scaler.transform(df_train[['age', 'bmi', 'charges']])\n",
        "df_test[['age', 'bmi', 'charges']] = scaler.transform(df_test[['age', 'bmi', 'charges']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Cawq4yKb4Ukg"
      },
      "outputs": [],
      "source": [
        "y_train = df_train.pop(\"charges\")\n",
        "y_test = df_test.pop(\"charges\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfJFd-144RDf"
      },
      "source": [
        "### Задание №5\n",
        "Реализуйте функцию, считающую `MSE` метрику.\n",
        "\n",
        "Ваша функция должна уметь принимать torch.Tensor, numpy-массивы и pd.Series."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Используем устройство cpu\n"
          ]
        }
      ],
      "source": [
        "# Определяем устройство: если доступен GPU, используем cuda, иначе CPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(\"Используем устройство\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Skg0gGe-4RDf"
      },
      "outputs": [],
      "source": [
        "def metric(preds, y):\n",
        "\n",
        "    for i in (preds, y):\n",
        "        if type(i) == np.ndarray:\n",
        "            i = torch.from_numpy(preds)\n",
        "        elif type(i) == pd.Series:\n",
        "            i = torch.tensor(i.values)\n",
        "    MSE = sum(((preds - y)**2) / y.shape[0])\n",
        "    return MSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BYIU8bJ4RDf"
      },
      "source": [
        "### Задание №6\n",
        "Реализуйте бейзлайн на `LinearRegression` и `GradientBoostingRegressor`, отправьте метрики в ЛМС.\n",
        "\n",
        "Используйте гиперпараметры по-умолчанию в обоих моделях."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qsxn2Gux4RDf"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train.drop(columns=['sex', 'region', 'smoker'], axis=1, inplace=True)\n",
        "df_test.drop(columns=['sex', 'region', 'smoker'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "boost = GradientBoostingRegressor()\n",
        "linear = LinearRegression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "boost.fit(df_train, y_train)\n",
        "linear.fit(df_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.11499598580020742\n",
            "0.2230241963394205\n"
          ]
        }
      ],
      "source": [
        "print(metric(boost.predict(df_test), y_test))\n",
        "print(metric(linear.predict(df_test), y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnmMv2Dt4RDf"
      },
      "source": [
        "### Задание №7\n",
        "Вычислите среднее значение целевой переменной на тренировочной выборке (train).\n",
        "\n",
        "Подсчитайте MSE при константном предсказании этим средним и отправьте его в ЛМС."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_mean = np.mean(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "dummy_model = np.full_like(y_test, target_mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.115550592874472"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metric(target_mean, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOG4gI6l4RDg"
      },
      "source": [
        "### Задание №8\n",
        "Создайте сеть, состоящую из одного слоя Linear, залейте в лмс код, который в `model` запишет вашу модель\n",
        "\n",
        "В качестве признаков используйте все колонки в текущем датасете, за исключением таргета"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "U-e--4Pv4RDg"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def build_model():\n",
        "\n",
        "    mod = nn.Linear(in_features=11, out_features=1)\n",
        "    return mod\n",
        "\n",
        "model = build_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "0keux04D4RDg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "torch.random.manual_seed(seed)\n",
        "t_x_train = torch.from_numpy(df_train.to_numpy().astype(float)).to(dtype=torch.float32)\n",
        "t_y_train = torch.from_numpy(y_train.to_numpy()).to(dtype=torch.float32).view(-1, 1)\n",
        "t_x_test = torch.from_numpy(df_test.to_numpy().astype(float)).to(dtype=torch.float32)\n",
        "t_y_test = torch.from_numpy(y_test.to_numpy()).to(dtype=torch.float32).view(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1071, 11])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t_x_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1071, 1])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t_y_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VR-vYZaR4RDg"
      },
      "source": [
        "### Задание №9\n",
        "Напишите функцию `train_loop`, которая будет учить модель по данным на 2к итераций.\n",
        "Считайте, что данные уже хранятся в переменных `t_x_train`, `t_y_train`.\n",
        "Ваша функция `train_loop` должна вернуть список из лоссов на каждой итерации (т.е. список длины 2000).\n",
        "\n",
        "Используйте `learning_rate=1e-2` в оптимизаторе.\n",
        "\n",
        "Для простоты за одну итерацию делайте проход вперед и проход назад на всех наших обучающих данных.\n",
        "Это будет полный градиентный спуск (не по батчам) - можем себе позволить, данных немного.\n",
        "\n",
        "_Подсказка 1_: Вам не обязательно учить модель на видеокарте, CPU будет достаточно.\n",
        "\n",
        "_Подсказка 2_: `tqdm` - это библиотека, которая рисует прогресс итераций"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "OlmWls5Q4RDg"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Глобальные переменные t_x_train, t_y_train должны быть доступны в среде\n",
        "def train_loop(model: nn.Module) -> list[float]:\n",
        "    losses = []\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "\n",
        "    for _ in tqdm(range(2000)):  # Прогресс-бар для наглядности\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(t_x_train)\n",
        "        loss = criterion(predictions, t_y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        losses.append(loss.item())  # Сохраняем loss для каждой итерации\n",
        "    \n",
        "    return losses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltF7ePyS4RDg"
      },
      "source": [
        "### Задание №10\n",
        "Обучите модель, состоящую из одного слоя `Linear`.\n",
        "Приложите в ЛМС метрику `MSE` на тестовых данных.\n",
        "Используйте `learning_rate=1e-2` в оптимизаторе.\n",
        "\n",
        "Когда будете тестировать, не забудьте перенести тестовые данные в `torch.Tensor`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Z8eHxjwM4RDg"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [00:00<00:00, 2148.29it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[1.27213454246521,\n",
              " 1.2554354667663574,\n",
              " 1.24014413356781,\n",
              " 1.2260209321975708,\n",
              " 1.2130261659622192,\n",
              " 1.201027750968933,\n",
              " 1.1899504661560059,\n",
              " 1.1796804666519165,\n",
              " 1.1701879501342773,\n",
              " 1.1613634824752808,\n",
              " 1.1531615257263184,\n",
              " 1.1454819440841675,\n",
              " 1.138307809829712,\n",
              " 1.1315735578536987,\n",
              " 1.1252400875091553,\n",
              " 1.119249939918518,\n",
              " 1.11359703540802,\n",
              " 1.1082377433776855,\n",
              " 1.1031432151794434,\n",
              " 1.098306655883789,\n",
              " 1.0937085151672363,\n",
              " 1.0893282890319824,\n",
              " 1.0851669311523438,\n",
              " 1.0811935663223267,\n",
              " 1.0773717164993286,\n",
              " 1.0736899375915527,\n",
              " 1.070155143737793,\n",
              " 1.0667905807495117,\n",
              " 1.0635557174682617,\n",
              " 1.0604358911514282,\n",
              " 1.0574589967727661,\n",
              " 1.054590106010437,\n",
              " 1.0518134832382202,\n",
              " 1.0491206645965576,\n",
              " 1.0464991331100464,\n",
              " 1.0439547300338745,\n",
              " 1.041480541229248,\n",
              " 1.0390862226486206,\n",
              " 1.036756992340088,\n",
              " 1.0345004796981812,\n",
              " 1.0322985649108887,\n",
              " 1.0301318168640137,\n",
              " 1.0280269384384155,\n",
              " 1.0259881019592285,\n",
              " 1.0239949226379395,\n",
              " 1.0220483541488647,\n",
              " 1.020148754119873,\n",
              " 1.018275499343872,\n",
              " 1.0164411067962646,\n",
              " 1.0146440267562866,\n",
              " 1.012872576713562,\n",
              " 1.0111225843429565,\n",
              " 1.009398341178894,\n",
              " 1.007705807685852,\n",
              " 1.0060293674468994,\n",
              " 1.004370927810669,\n",
              " 1.0027258396148682,\n",
              " 1.0010994672775269,\n",
              " 0.9994902014732361,\n",
              " 0.997894823551178,\n",
              " 0.9963246583938599,\n",
              " 0.9947702288627625,\n",
              " 0.9932339191436768,\n",
              " 0.991703987121582,\n",
              " 0.9901772737503052,\n",
              " 0.9886683225631714,\n",
              " 0.9871699810028076,\n",
              " 0.9856722950935364,\n",
              " 0.9841737151145935,\n",
              " 0.9826837778091431,\n",
              " 0.9812042713165283,\n",
              " 0.9797359704971313,\n",
              " 0.9782812595367432,\n",
              " 0.9768415093421936,\n",
              " 0.975399911403656,\n",
              " 0.9739521741867065,\n",
              " 0.9725117087364197,\n",
              " 0.9710682034492493,\n",
              " 0.9696435928344727,\n",
              " 0.9682179689407349,\n",
              " 0.9667936563491821,\n",
              " 0.9653735160827637,\n",
              " 0.9639477729797363,\n",
              " 0.9625089168548584,\n",
              " 0.9610527157783508,\n",
              " 0.9595714807510376,\n",
              " 0.9580732583999634,\n",
              " 0.9565703272819519,\n",
              " 0.9550572037696838,\n",
              " 0.9535319805145264,\n",
              " 0.9519975185394287,\n",
              " 0.9504599571228027,\n",
              " 0.9489266276359558,\n",
              " 0.9473955631256104,\n",
              " 0.9458541870117188,\n",
              " 0.9442867040634155,\n",
              " 0.9427009224891663,\n",
              " 0.9410946369171143,\n",
              " 0.9394811391830444,\n",
              " 0.9378674626350403,\n",
              " 0.9362432360649109,\n",
              " 0.9346050024032593,\n",
              " 0.9329583644866943,\n",
              " 0.9312947988510132,\n",
              " 0.9296146631240845,\n",
              " 0.9279125332832336,\n",
              " 0.9262027144432068,\n",
              " 0.9244781136512756,\n",
              " 0.9227449893951416,\n",
              " 0.9210111498832703,\n",
              " 0.9192699790000916,\n",
              " 0.9175074100494385,\n",
              " 0.9157212972640991,\n",
              " 0.9139285683631897,\n",
              " 0.9121320843696594,\n",
              " 0.9102914333343506,\n",
              " 0.9084535241127014,\n",
              " 0.9066228270530701,\n",
              " 0.9047878384590149,\n",
              " 0.90293288230896,\n",
              " 0.9010744094848633,\n",
              " 0.8991901874542236,\n",
              " 0.8972536325454712,\n",
              " 0.895317018032074,\n",
              " 0.8933776617050171,\n",
              " 0.8914430141448975,\n",
              " 0.8894971013069153,\n",
              " 0.8875088691711426,\n",
              " 0.8854859471321106,\n",
              " 0.8834340572357178,\n",
              " 0.881379246711731,\n",
              " 0.879314124584198,\n",
              " 0.8772187829017639,\n",
              " 0.8751032948493958,\n",
              " 0.8729644417762756,\n",
              " 0.870810866355896,\n",
              " 0.8686398267745972,\n",
              " 0.8664585947990417,\n",
              " 0.8642740845680237,\n",
              " 0.8620601892471313,\n",
              " 0.8598228096961975,\n",
              " 0.8575677871704102,\n",
              " 0.8552883863449097,\n",
              " 0.8530128002166748,\n",
              " 0.8507824540138245,\n",
              " 0.8485527038574219,\n",
              " 0.846294641494751,\n",
              " 0.8440168499946594,\n",
              " 0.8417372107505798,\n",
              " 0.8394653797149658,\n",
              " 0.8371758460998535,\n",
              " 0.8348863124847412,\n",
              " 0.832591712474823,\n",
              " 0.8302778005599976,\n",
              " 0.8279372453689575,\n",
              " 0.8255919814109802,\n",
              " 0.8232545852661133,\n",
              " 0.8209187388420105,\n",
              " 0.8185859322547913,\n",
              " 0.8162372708320618,\n",
              " 0.8138766884803772,\n",
              " 0.8115032911300659,\n",
              " 0.8091128468513489,\n",
              " 0.8067225217819214,\n",
              " 0.8043453097343445,\n",
              " 0.8019768595695496,\n",
              " 0.7996069192886353,\n",
              " 0.79722660779953,\n",
              " 0.7948421239852905,\n",
              " 0.7924433350563049,\n",
              " 0.7900070548057556,\n",
              " 0.7875504493713379,\n",
              " 0.7850555777549744,\n",
              " 0.7825514078140259,\n",
              " 0.7800299525260925,\n",
              " 0.7774963974952698,\n",
              " 0.774946391582489,\n",
              " 0.7723888754844666,\n",
              " 0.7698275446891785,\n",
              " 0.7672749161720276,\n",
              " 0.7647169232368469,\n",
              " 0.7621355652809143,\n",
              " 0.7595401406288147,\n",
              " 0.7569320201873779,\n",
              " 0.7543140053749084,\n",
              " 0.7516801357269287,\n",
              " 0.7490410208702087,\n",
              " 0.7463878393173218,\n",
              " 0.7437209486961365,\n",
              " 0.741026759147644,\n",
              " 0.7383275032043457,\n",
              " 0.7355919480323792,\n",
              " 0.7328557968139648,\n",
              " 0.7301164269447327,\n",
              " 0.7273545265197754,\n",
              " 0.7245762944221497,\n",
              " 0.7218088507652283,\n",
              " 0.7190207839012146,\n",
              " 0.7162330150604248,\n",
              " 0.7134414911270142,\n",
              " 0.7106460332870483,\n",
              " 0.7078097462654114,\n",
              " 0.7049578428268433,\n",
              " 0.7020933628082275,\n",
              " 0.6991912722587585,\n",
              " 0.696264922618866,\n",
              " 0.693344235420227,\n",
              " 0.6904155015945435,\n",
              " 0.6874664425849915,\n",
              " 0.6845049858093262,\n",
              " 0.6815106272697449,\n",
              " 0.6785171627998352,\n",
              " 0.6755189299583435,\n",
              " 0.6725131273269653,\n",
              " 0.6694898009300232,\n",
              " 0.666448175907135,\n",
              " 0.6634076237678528,\n",
              " 0.6603521704673767,\n",
              " 0.657280683517456,\n",
              " 0.6542023420333862,\n",
              " 0.651105523109436,\n",
              " 0.647990345954895,\n",
              " 0.6448776721954346,\n",
              " 0.6417605876922607,\n",
              " 0.6386194825172424,\n",
              " 0.6354361772537231,\n",
              " 0.6322144865989685,\n",
              " 0.6289992332458496,\n",
              " 0.6257950067520142,\n",
              " 0.622593104839325,\n",
              " 0.6193749904632568,\n",
              " 0.6161104440689087,\n",
              " 0.6128477454185486,\n",
              " 0.609563410282135,\n",
              " 0.6062319278717041,\n",
              " 0.6028653979301453,\n",
              " 0.5994738340377808,\n",
              " 0.5960555672645569,\n",
              " 0.5926179885864258,\n",
              " 0.5891825556755066,\n",
              " 0.5857501029968262,\n",
              " 0.5823102593421936,\n",
              " 0.57887864112854,\n",
              " 0.5754555463790894,\n",
              " 0.5719838738441467,\n",
              " 0.5684771537780762,\n",
              " 0.5650087594985962,\n",
              " 0.5615530610084534,\n",
              " 0.558108925819397,\n",
              " 0.5546646118164062,\n",
              " 0.5512121319770813,\n",
              " 0.547722339630127,\n",
              " 0.5442500710487366,\n",
              " 0.5408069491386414,\n",
              " 0.537362277507782,\n",
              " 0.5339133739471436,\n",
              " 0.5304498672485352,\n",
              " 0.5269699096679688,\n",
              " 0.5235115885734558,\n",
              " 0.5200710892677307,\n",
              " 0.5166391730308533,\n",
              " 0.513225257396698,\n",
              " 0.5098320245742798,\n",
              " 0.5064499974250793,\n",
              " 0.5030888319015503,\n",
              " 0.4997430443763733,\n",
              " 0.49641287326812744,\n",
              " 0.49310991168022156,\n",
              " 0.48983123898506165,\n",
              " 0.4865824580192566,\n",
              " 0.4833560883998871,\n",
              " 0.48014864325523376,\n",
              " 0.47696325182914734,\n",
              " 0.47379690408706665,\n",
              " 0.47062429785728455,\n",
              " 0.4674820005893707,\n",
              " 0.46437257528305054,\n",
              " 0.46128466725349426,\n",
              " 0.4582330584526062,\n",
              " 0.4552273154258728,\n",
              " 0.45225200057029724,\n",
              " 0.4493107795715332,\n",
              " 0.4463990032672882,\n",
              " 0.44352370500564575,\n",
              " 0.4406715929508209,\n",
              " 0.4378548264503479,\n",
              " 0.4350734055042267,\n",
              " 0.4323157072067261,\n",
              " 0.4295869767665863,\n",
              " 0.4268937110900879,\n",
              " 0.4242323935031891,\n",
              " 0.42160311341285706,\n",
              " 0.41900691390037537,\n",
              " 0.4164392948150635,\n",
              " 0.4138851463794708,\n",
              " 0.41135814785957336,\n",
              " 0.40886905789375305,\n",
              " 0.4064052999019623,\n",
              " 0.4039734899997711,\n",
              " 0.40157586336135864,\n",
              " 0.3992130756378174,\n",
              " 0.3968951404094696,\n",
              " 0.39460551738739014,\n",
              " 0.39235612750053406,\n",
              " 0.39014914631843567,\n",
              " 0.38797199726104736,\n",
              " 0.3858230412006378,\n",
              " 0.38371095061302185,\n",
              " 0.3816469609737396,\n",
              " 0.37961000204086304,\n",
              " 0.37760332226753235,\n",
              " 0.3756268322467804,\n",
              " 0.37367820739746094,\n",
              " 0.37175896763801575,\n",
              " 0.36986833810806274,\n",
              " 0.3680104911327362,\n",
              " 0.3661852777004242,\n",
              " 0.3643917441368103,\n",
              " 0.36262819170951843,\n",
              " 0.3608998656272888,\n",
              " 0.35919490456581116,\n",
              " 0.3575102388858795,\n",
              " 0.35584938526153564,\n",
              " 0.3542151153087616,\n",
              " 0.35260263085365295,\n",
              " 0.3510124385356903,\n",
              " 0.34944435954093933,\n",
              " 0.3478991389274597,\n",
              " 0.34637677669525146,\n",
              " 0.34488368034362793,\n",
              " 0.3434114456176758,\n",
              " 0.3419564962387085,\n",
              " 0.3405267596244812,\n",
              " 0.33911579847335815,\n",
              " 0.3377258777618408,\n",
              " 0.33635732531547546,\n",
              " 0.3350077271461487,\n",
              " 0.33367934823036194,\n",
              " 0.33237704634666443,\n",
              " 0.33109983801841736,\n",
              " 0.32984402775764465,\n",
              " 0.32860836386680603,\n",
              " 0.32739466428756714,\n",
              " 0.3262034058570862,\n",
              " 0.32503318786621094,\n",
              " 0.3238825798034668,\n",
              " 0.3227507770061493,\n",
              " 0.32163912057876587,\n",
              " 0.32054445147514343,\n",
              " 0.3194674253463745,\n",
              " 0.318406343460083,\n",
              " 0.31736305356025696,\n",
              " 0.3163374662399292,\n",
              " 0.3153313100337982,\n",
              " 0.3143419027328491,\n",
              " 0.31336963176727295,\n",
              " 0.3124130070209503,\n",
              " 0.31147199869155884,\n",
              " 0.31054651737213135,\n",
              " 0.3096376061439514,\n",
              " 0.30874472856521606,\n",
              " 0.3078663647174835,\n",
              " 0.30700889229774475,\n",
              " 0.3061739504337311,\n",
              " 0.3053579032421112,\n",
              " 0.3045583665370941,\n",
              " 0.3037727177143097,\n",
              " 0.3030003309249878,\n",
              " 0.3022409677505493,\n",
              " 0.30149397253990173,\n",
              " 0.3007584512233734,\n",
              " 0.3000362515449524,\n",
              " 0.2993271052837372,\n",
              " 0.2986299693584442,\n",
              " 0.29794442653656006,\n",
              " 0.2972705364227295,\n",
              " 0.29660817980766296,\n",
              " 0.2959572374820709,\n",
              " 0.2953174412250519,\n",
              " 0.29468852281570435,\n",
              " 0.29407039284706116,\n",
              " 0.2934630811214447,\n",
              " 0.2928658723831177,\n",
              " 0.29227763414382935,\n",
              " 0.29169949889183044,\n",
              " 0.29113128781318665,\n",
              " 0.29057279229164124,\n",
              " 0.2900238037109375,\n",
              " 0.28948426246643066,\n",
              " 0.28895413875579834,\n",
              " 0.28843292593955994,\n",
              " 0.2879192531108856,\n",
              " 0.2874143421649933,\n",
              " 0.28691819310188293,\n",
              " 0.2864305078983307,\n",
              " 0.2859492301940918,\n",
              " 0.28547582030296326,\n",
              " 0.2850094437599182,\n",
              " 0.2845527231693268,\n",
              " 0.2841094434261322,\n",
              " 0.28367501497268677,\n",
              " 0.2832512855529785,\n",
              " 0.2828347384929657,\n",
              " 0.2824254035949707,\n",
              " 0.28202176094055176,\n",
              " 0.2816242575645447,\n",
              " 0.28123342990875244,\n",
              " 0.2808489501476288,\n",
              " 0.280469685792923,\n",
              " 0.28009703755378723,\n",
              " 0.27973100543022156,\n",
              " 0.27937114238739014,\n",
              " 0.2790175974369049,\n",
              " 0.2786701023578644,\n",
              " 0.2783287465572357,\n",
              " 0.27799439430236816,\n",
              " 0.2776700258255005,\n",
              " 0.27735114097595215,\n",
              " 0.2770385444164276,\n",
              " 0.2767321765422821,\n",
              " 0.2764308452606201,\n",
              " 0.2761368453502655,\n",
              " 0.27584755420684814,\n",
              " 0.2755639851093292,\n",
              " 0.27528589963912964,\n",
              " 0.2750122845172882,\n",
              " 0.2747429609298706,\n",
              " 0.274478018283844,\n",
              " 0.2742171287536621,\n",
              " 0.2739604115486145,\n",
              " 0.27370771765708923,\n",
              " 0.2734590172767639,\n",
              " 0.2732142210006714,\n",
              " 0.2729759216308594,\n",
              " 0.2727416157722473,\n",
              " 0.27251118421554565,\n",
              " 0.27228423953056335,\n",
              " 0.27206090092658997,\n",
              " 0.27184104919433594,\n",
              " 0.27162453532218933,\n",
              " 0.2714115381240845,\n",
              " 0.27120453119277954,\n",
              " 0.27100053429603577,\n",
              " 0.270799458026886,\n",
              " 0.27060139179229736,\n",
              " 0.27040618658065796,\n",
              " 0.27021387219429016,\n",
              " 0.2700239419937134,\n",
              " 0.26983609795570374,\n",
              " 0.2696498930454254,\n",
              " 0.2694664001464844,\n",
              " 0.26928651332855225,\n",
              " 0.26911118626594543,\n",
              " 0.2689383029937744,\n",
              " 0.26876771450042725,\n",
              " 0.2685994505882263,\n",
              " 0.26843345165252686,\n",
              " 0.26826944947242737,\n",
              " 0.2681069076061249,\n",
              " 0.2679460644721985,\n",
              " 0.2677873373031616,\n",
              " 0.26763105392456055,\n",
              " 0.2674773931503296,\n",
              " 0.26732608675956726,\n",
              " 0.2671777606010437,\n",
              " 0.2670312523841858,\n",
              " 0.2668865919113159,\n",
              " 0.26674357056617737,\n",
              " 0.2666032016277313,\n",
              " 0.26646658778190613,\n",
              " 0.26633161306381226,\n",
              " 0.26619821786880493,\n",
              " 0.266066312789917,\n",
              " 0.2659359872341156,\n",
              " 0.2658071517944336,\n",
              " 0.26567986607551575,\n",
              " 0.2655545175075531,\n",
              " 0.26543059945106506,\n",
              " 0.26530763506889343,\n",
              " 0.2651878595352173,\n",
              " 0.26506951451301575,\n",
              " 0.26495254039764404,\n",
              " 0.2648366689682007,\n",
              " 0.2647218406200409,\n",
              " 0.2646081745624542,\n",
              " 0.26449570059776306,\n",
              " 0.2643841803073883,\n",
              " 0.26427382230758667,\n",
              " 0.2641650438308716,\n",
              " 0.26405733823776245,\n",
              " 0.2639504075050354,\n",
              " 0.26384440064430237,\n",
              " 0.26373937726020813,\n",
              " 0.2636353671550751,\n",
              " 0.2635323405265808,\n",
              " 0.26343122124671936,\n",
              " 0.26333290338516235,\n",
              " 0.263235867023468,\n",
              " 0.2631397247314453,\n",
              " 0.26304441690444946,\n",
              " 0.26295003294944763,\n",
              " 0.26285648345947266,\n",
              " 0.2627637982368469,\n",
              " 0.26267209649086,\n",
              " 0.2625811696052551,\n",
              " 0.26249104738235474,\n",
              " 0.26240167021751404,\n",
              " 0.2623130977153778,\n",
              " 0.2622252106666565,\n",
              " 0.26213812828063965,\n",
              " 0.2620517313480377,\n",
              " 0.2619660198688507,\n",
              " 0.26188087463378906,\n",
              " 0.26179635524749756,\n",
              " 0.26171255111694336,\n",
              " 0.2616296112537384,\n",
              " 0.26154714822769165,\n",
              " 0.2614651024341583,\n",
              " 0.26138365268707275,\n",
              " 0.26130279898643494,\n",
              " 0.26122257113456726,\n",
              " 0.2611425518989563,\n",
              " 0.26106294989585876,\n",
              " 0.2609845697879791,\n",
              " 0.26090696454048157,\n",
              " 0.26083067059516907,\n",
              " 0.26075536012649536,\n",
              " 0.260680615901947,\n",
              " 0.26060643792152405,\n",
              " 0.2605327069759369,\n",
              " 0.2604590654373169,\n",
              " 0.260384738445282,\n",
              " 0.26031073927879333,\n",
              " 0.2602362632751465,\n",
              " 0.2601615786552429,\n",
              " 0.2600872814655304,\n",
              " 0.26001277565956116,\n",
              " 0.25993990898132324,\n",
              " 0.25986751914024353,\n",
              " 0.25979557633399963,\n",
              " 0.2597240209579468,\n",
              " 0.2596529722213745,\n",
              " 0.2595823109149933,\n",
              " 0.25951144099235535,\n",
              " 0.25943994522094727,\n",
              " 0.2593689262866974,\n",
              " 0.25929826498031616,\n",
              " 0.25922805070877075,\n",
              " 0.2591584324836731,\n",
              " 0.25908926129341125,\n",
              " 0.25902053713798523,\n",
              " 0.25895217061042786,\n",
              " 0.2588841915130615,\n",
              " 0.25881657004356384,\n",
              " 0.25874921679496765,\n",
              " 0.2586820423603058,\n",
              " 0.25861379504203796,\n",
              " 0.25854599475860596,\n",
              " 0.2584784924983978,\n",
              " 0.2584114372730255,\n",
              " 0.2583446800708771,\n",
              " 0.25827786326408386,\n",
              " 0.25820955634117126,\n",
              " 0.25814172625541687,\n",
              " 0.2580741345882416,\n",
              " 0.25800442695617676,\n",
              " 0.2579331696033478,\n",
              " 0.2578622102737427,\n",
              " 0.2577916085720062,\n",
              " 0.25772130489349365,\n",
              " 0.25765132904052734,\n",
              " 0.25758153200149536,\n",
              " 0.25751206278800964,\n",
              " 0.25744280219078064,\n",
              " 0.2573738694190979,\n",
              " 0.2573045790195465,\n",
              " 0.25723448395729065,\n",
              " 0.25716471672058105,\n",
              " 0.2570953965187073,\n",
              " 0.25702622532844543,\n",
              " 0.256955087184906,\n",
              " 0.25688454508781433,\n",
              " 0.2568143308162689,\n",
              " 0.25674447417259216,\n",
              " 0.2566748559474945,\n",
              " 0.25660526752471924,\n",
              " 0.25653284788131714,\n",
              " 0.25646063685417175,\n",
              " 0.25638875365257263,\n",
              " 0.2563171982765198,\n",
              " 0.25624603033065796,\n",
              " 0.2561752200126648,\n",
              " 0.25610464811325073,\n",
              " 0.2560344338417053,\n",
              " 0.2559645473957062,\n",
              " 0.2558949291706085,\n",
              " 0.2558256685733795,\n",
              " 0.25575754046440125,\n",
              " 0.2556895911693573,\n",
              " 0.25562137365341187,\n",
              " 0.25555339455604553,\n",
              " 0.2554857134819031,\n",
              " 0.25541868805885315,\n",
              " 0.2553521692752838,\n",
              " 0.25528597831726074,\n",
              " 0.25522008538246155,\n",
              " 0.25515446066856384,\n",
              " 0.2550892233848572,\n",
              " 0.2550240457057953,\n",
              " 0.2549588680267334,\n",
              " 0.2548939287662506,\n",
              " 0.2548292577266693,\n",
              " 0.25476399064064026,\n",
              " 0.25469502806663513,\n",
              " 0.25462639331817627,\n",
              " 0.25455814599990845,\n",
              " 0.25448986887931824,\n",
              " 0.2544187307357788,\n",
              " 0.25434786081314087,\n",
              " 0.2542773485183716,\n",
              " 0.25420716404914856,\n",
              " 0.2541372776031494,\n",
              " 0.254067987203598,\n",
              " 0.25399887561798096,\n",
              " 0.2539297342300415,\n",
              " 0.25386080145835876,\n",
              " 0.2537921071052551,\n",
              " 0.2537238299846649,\n",
              " 0.2536558210849762,\n",
              " 0.25358524918556213,\n",
              " 0.25350984930992126,\n",
              " 0.253434956073761,\n",
              " 0.2533605992794037,\n",
              " 0.25328660011291504,\n",
              " 0.25321194529533386,\n",
              " 0.25313523411750793,\n",
              " 0.2530588209629059,\n",
              " 0.2529827356338501,\n",
              " 0.25290700793266296,\n",
              " 0.2528316080570221,\n",
              " 0.2527565062046051,\n",
              " 0.2526817321777344,\n",
              " 0.2526073455810547,\n",
              " 0.2525333762168884,\n",
              " 0.2524597644805908,\n",
              " 0.2523864805698395,\n",
              " 0.2523135840892792,\n",
              " 0.2522408068180084,\n",
              " 0.2521667182445526,\n",
              " 0.2520929276943207,\n",
              " 0.252019464969635,\n",
              " 0.2519463300704956,\n",
              " 0.25187116861343384,\n",
              " 0.2517903745174408,\n",
              " 0.2517096698284149,\n",
              " 0.25162816047668457,\n",
              " 0.25154629349708557,\n",
              " 0.25146234035491943,\n",
              " 0.25137677788734436,\n",
              " 0.251288503408432,\n",
              " 0.251200795173645,\n",
              " 0.2511134743690491,\n",
              " 0.25102582573890686,\n",
              " 0.2509349584579468,\n",
              " 0.25084540247917175,\n",
              " 0.2507580816745758,\n",
              " 0.2506713271141052,\n",
              " 0.2505851984024048,\n",
              " 0.25049999356269836,\n",
              " 0.25041311979293823,\n",
              " 0.25032681226730347,\n",
              " 0.2502409815788269,\n",
              " 0.25015392899513245,\n",
              " 0.25006046891212463,\n",
              " 0.24995730817317963,\n",
              " 0.24985043704509735,\n",
              " 0.2497442364692688,\n",
              " 0.2496386617422104,\n",
              " 0.2495337426662445,\n",
              " 0.24942953884601593,\n",
              " 0.2493261694908142,\n",
              " 0.24922360479831696,\n",
              " 0.24912217259407043,\n",
              " 0.24902278184890747,\n",
              " 0.24892385303974152,\n",
              " 0.24882552027702332,\n",
              " 0.2487277239561081,\n",
              " 0.24863021075725555,\n",
              " 0.24853472411632538,\n",
              " 0.24843984842300415,\n",
              " 0.2483455240726471,\n",
              " 0.24825087189674377,\n",
              " 0.24815548956394196,\n",
              " 0.24806080758571625,\n",
              " 0.24796685576438904,\n",
              " 0.2478734254837036,\n",
              " 0.2477806806564331,\n",
              " 0.24768872559070587,\n",
              " 0.24759726226329803,\n",
              " 0.24750633537769318,\n",
              " 0.2474159151315689,\n",
              " 0.24732254445552826,\n",
              " 0.2472303956747055,\n",
              " 0.2471388429403305,\n",
              " 0.24704785645008087,\n",
              " 0.24695561826229095,\n",
              " 0.2468608170747757,\n",
              " 0.246766597032547,\n",
              " 0.24667292833328247,\n",
              " 0.24657991528511047,\n",
              " 0.2464873492717743,\n",
              " 0.24639518558979034,\n",
              " 0.24630358815193176,\n",
              " 0.24621257185935974,\n",
              " 0.24612221121788025,\n",
              " 0.24603109061717987,\n",
              " 0.24593685567378998,\n",
              " 0.24584323167800903,\n",
              " 0.24575015902519226,\n",
              " 0.2456575334072113,\n",
              " 0.24556545913219452,\n",
              " 0.24547390639781952,\n",
              " 0.2453848123550415,\n",
              " 0.24529863893985748,\n",
              " 0.2452133297920227,\n",
              " 0.24512764811515808,\n",
              " 0.24504315853118896,\n",
              " 0.2449592649936676,\n",
              " 0.24487632513046265,\n",
              " 0.24479320645332336,\n",
              " 0.24470928311347961,\n",
              " 0.24462582170963287,\n",
              " 0.24454250931739807,\n",
              " 0.24445679783821106,\n",
              " 0.24437052011489868,\n",
              " 0.24428293108940125,\n",
              " 0.24418430030345917,\n",
              " 0.24408648908138275,\n",
              " 0.24398940801620483,\n",
              " 0.2438930720090866,\n",
              " 0.2437974363565445,\n",
              " 0.24369938671588898,\n",
              " 0.24359606206417084,\n",
              " 0.2434932291507721,\n",
              " 0.24339018762111664,\n",
              " 0.24328704178333282,\n",
              " 0.24318233132362366,\n",
              " 0.24307791888713837,\n",
              " 0.24297215044498444,\n",
              " 0.24285458028316498,\n",
              " 0.24273781478405,\n",
              " 0.24261821806430817,\n",
              " 0.24249957501888275,\n",
              " 0.2423793524503708,\n",
              " 0.24225680530071259,\n",
              " 0.24213506281375885,\n",
              " 0.24201494455337524,\n",
              " 0.24189700186252594,\n",
              " 0.2417801469564438,\n",
              " 0.24166448414325714,\n",
              " 0.24155019223690033,\n",
              " 0.24143721163272858,\n",
              " 0.2413252294063568,\n",
              " 0.24121378362178802,\n",
              " 0.24110080301761627,\n",
              " 0.24098871648311615,\n",
              " 0.2408774346113205,\n",
              " 0.2407669872045517,\n",
              " 0.24065734446048737,\n",
              " 0.24054844677448273,\n",
              " 0.24043551087379456,\n",
              " 0.2403208613395691,\n",
              " 0.24020913243293762,\n",
              " 0.2400982826948166,\n",
              " 0.2399914562702179,\n",
              " 0.2398880273103714,\n",
              " 0.23978225886821747,\n",
              " 0.23967744410037994,\n",
              " 0.2395736426115036,\n",
              " 0.23947228491306305,\n",
              " 0.23937182128429413,\n",
              " 0.23927217721939087,\n",
              " 0.23917554318904877,\n",
              " 0.2390732318162918,\n",
              " 0.2389608770608902,\n",
              " 0.23884573578834534,\n",
              " 0.2387317419052124,\n",
              " 0.23861883580684662,\n",
              " 0.23850680887699127,\n",
              " 0.23839238286018372,\n",
              " 0.23827558755874634,\n",
              " 0.23815977573394775,\n",
              " 0.238044872879982,\n",
              " 0.23792216181755066,\n",
              " 0.23779623210430145,\n",
              " 0.2376706451177597,\n",
              " 0.2375449240207672,\n",
              " 0.23741945624351501,\n",
              " 0.2372884601354599,\n",
              " 0.23715901374816895,\n",
              " 0.23703083395957947,\n",
              " 0.23690447211265564,\n",
              " 0.23677782714366913,\n",
              " 0.23665229976177216,\n",
              " 0.23652797937393188,\n",
              " 0.23640292882919312,\n",
              " 0.23627470433712006,\n",
              " 0.23614485561847687,\n",
              " 0.2360125482082367,\n",
              " 0.2358814924955368,\n",
              " 0.23575156927108765,\n",
              " 0.2356228232383728,\n",
              " 0.23549522459506989,\n",
              " 0.23536871373653412,\n",
              " 0.23524357378482819,\n",
              " 0.2351202815771103,\n",
              " 0.2349981665611267,\n",
              " 0.23487722873687744,\n",
              " 0.2347572296857834,\n",
              " 0.23463772237300873,\n",
              " 0.23451575636863708,\n",
              " 0.23439186811447144,\n",
              " 0.23426899313926697,\n",
              " 0.23414313793182373,\n",
              " 0.23401948809623718,\n",
              " 0.2338976263999939,\n",
              " 0.23377682268619537,\n",
              " 0.2336571216583252,\n",
              " 0.23353856801986694,\n",
              " 0.23342129588127136,\n",
              " 0.2333049774169922,\n",
              " 0.2331870198249817,\n",
              " 0.23306700587272644,\n",
              " 0.23294809460639954,\n",
              " 0.2328307181596756,\n",
              " 0.2327142357826233,\n",
              " 0.23259881138801575,\n",
              " 0.2324792742729187,\n",
              " 0.23235918581485748,\n",
              " 0.2322375625371933,\n",
              " 0.23211713135242462,\n",
              " 0.2319972813129425,\n",
              " 0.2318742722272873,\n",
              " 0.23175227642059326,\n",
              " 0.23163163661956787,\n",
              " 0.23151150345802307,\n",
              " 0.23138582706451416,\n",
              " 0.23126082122325897,\n",
              " 0.23113557696342468,\n",
              " 0.23101197183132172,\n",
              " 0.23088961839675903,\n",
              " 0.23076672852039337,\n",
              " 0.23063473403453827,\n",
              " 0.2305029183626175,\n",
              " 0.2303723245859146,\n",
              " 0.23024265468120575,\n",
              " 0.2301120162010193,\n",
              " 0.22998258471488953,\n",
              " 0.2298543006181717,\n",
              " 0.2297271490097046,\n",
              " 0.22960101068019867,\n",
              " 0.22947563230991364,\n",
              " 0.22935020923614502,\n",
              " 0.229227215051651,\n",
              " 0.2291051298379898,\n",
              " 0.22898393869400024,\n",
              " 0.22886237502098083,\n",
              " 0.22874027490615845,\n",
              " 0.22861917316913605,\n",
              " 0.22849904000759125,\n",
              " 0.22837992012500763,\n",
              " 0.22826115787029266,\n",
              " 0.22813798487186432,\n",
              " 0.22801107168197632,\n",
              " 0.2278851866722107,\n",
              " 0.2277597039937973,\n",
              " 0.2276313602924347,\n",
              " 0.22750194370746613,\n",
              " 0.22737357020378113,\n",
              " 0.2272462695837021,\n",
              " 0.22711989283561707,\n",
              " 0.22699426114559174,\n",
              " 0.22686974704265594,\n",
              " 0.2267465740442276,\n",
              " 0.22662551701068878,\n",
              " 0.2265053540468216,\n",
              " 0.22638539969921112,\n",
              " 0.2262638807296753,\n",
              " 0.2261434644460678,\n",
              " 0.2260238230228424,\n",
              " 0.22590340673923492,\n",
              " 0.22578009963035583,\n",
              " 0.22565780580043793,\n",
              " 0.22553475201129913,\n",
              " 0.22540898621082306,\n",
              " 0.22528381645679474,\n",
              " 0.22515961527824402,\n",
              " 0.22503602504730225,\n",
              " 0.2249133437871933,\n",
              " 0.22479163110256195,\n",
              " 0.22467100620269775,\n",
              " 0.22455136477947235,\n",
              " 0.22443412244319916,\n",
              " 0.2243189811706543,\n",
              " 0.22420617938041687,\n",
              " 0.2240941971540451,\n",
              " 0.22398367524147034,\n",
              " 0.22387468814849854,\n",
              " 0.22376638650894165,\n",
              " 0.2236587107181549,\n",
              " 0.22355176508426666,\n",
              " 0.22344551980495453,\n",
              " 0.22334110736846924,\n",
              " 0.22323867678642273,\n",
              " 0.22313690185546875,\n",
              " 0.2230357676744461,\n",
              " 0.22293215990066528,\n",
              " 0.22282543778419495,\n",
              " 0.2227194458246231,\n",
              " 0.22261415421962738,\n",
              " 0.22250941395759583,\n",
              " 0.22240065038204193,\n",
              " 0.2222927361726761,\n",
              " 0.22218550741672516,\n",
              " 0.22207897901535034,\n",
              " 0.22197388112545013,\n",
              " 0.22186946868896484,\n",
              " 0.22176586091518402,\n",
              " 0.22166290879249573,\n",
              " 0.22156070172786713,\n",
              " 0.22145618498325348,\n",
              " 0.22134912014007568,\n",
              " 0.22124123573303223,\n",
              " 0.22113068401813507,\n",
              " 0.22102071344852448,\n",
              " 0.2209113985300064,\n",
              " 0.2208036482334137,\n",
              " 0.22069787979125977,\n",
              " 0.2205929160118103,\n",
              " 0.22048862278461456,\n",
              " 0.22038500010967255,\n",
              " 0.22028250992298126,\n",
              " 0.22018206119537354,\n",
              " 0.22008220851421356,\n",
              " 0.21998293697834015,\n",
              " 0.21988427639007568,\n",
              " 0.21978625655174255,\n",
              " 0.2196882665157318,\n",
              " 0.21958760917186737,\n",
              " 0.21948842704296112,\n",
              " 0.21939100325107574,\n",
              " 0.21929417550563812,\n",
              " 0.21919776499271393,\n",
              " 0.21910212934017181,\n",
              " 0.21900691092014313,\n",
              " 0.21891219913959503,\n",
              " 0.21881748735904694,\n",
              " 0.21871939301490784,\n",
              " 0.2186218947172165,\n",
              " 0.21852485835552216,\n",
              " 0.21842841804027557,\n",
              " 0.21833278238773346,\n",
              " 0.2182394564151764,\n",
              " 0.2181466519832611,\n",
              " 0.21805419027805328,\n",
              " 0.21796198189258575,\n",
              " 0.21787026524543762,\n",
              " 0.21777905523777008,\n",
              " 0.21768775582313538,\n",
              " 0.21759295463562012,\n",
              " 0.21749889850616455,\n",
              " 0.21740572154521942,\n",
              " 0.21731305122375488,\n",
              " 0.21722091734409332,\n",
              " 0.21712926030158997,\n",
              " 0.217038094997406,\n",
              " 0.21694743633270264,\n",
              " 0.21685729920864105,\n",
              " 0.21676760911941528,\n",
              " 0.21667839586734772,\n",
              " 0.2165897786617279,\n",
              " 0.21650159358978271,\n",
              " 0.2164139449596405,\n",
              " 0.21632711589336395,\n",
              " 0.21624094247817993,\n",
              " 0.21615532040596008,\n",
              " 0.21607020497322083,\n",
              " 0.21598592400550842,\n",
              " 0.21590307354927063,\n",
              " 0.21582071483135223,\n",
              " 0.21573902666568756,\n",
              " 0.2156578153371811,\n",
              " 0.2155773937702179,\n",
              " 0.215498149394989,\n",
              " 0.21541868150234222,\n",
              " 0.21533970534801483,\n",
              " 0.21526262164115906,\n",
              " 0.2151879072189331,\n",
              " 0.21511361002922058,\n",
              " 0.2150396853685379,\n",
              " ...]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_loop(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Финальный MSE-лосс: 0.14732885360717773\n"
          ]
        }
      ],
      "source": [
        "# Считаем финальную метрику на тестовых данных\n",
        "with torch.no_grad():\n",
        "    # Помним, что все тензоры должны быть на одном устройстве\n",
        "    out = model(t_x_test)\n",
        "    mse_loss = F.mse_loss(out, t_y_test).item()\n",
        "    print(\"Финальный MSE-лосс:\", mse_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from torchmetrics.functional import mean_squared_error\n",
        "\n",
        "with torch.no_grad():\n",
        "    predictions = model(t_x_test)\n",
        "    mse = metric(predictions, t_y_test)\n",
        "\n",
        "# print(mse.item())  # Отправь этот результат в чекер"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.1473)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(mse) / len(mse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_387inA4RDg"
      },
      "source": [
        "### Задание №11\n",
        "Вам необходимо усложнить существующую нейронную сеть, добавив один скрытый слой.\n",
        "\n",
        "Используйте следующие параметры:\n",
        "\n",
        "Размерность скрытого слоя: 6 нейронов, функция активации -  `ReLU`\n",
        "\n",
        "Приложите в лмс код, который в переменную `model` запишет вашу модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "i6fS2zOE4RDg"
      },
      "outputs": [],
      "source": [
        "def build_model():\n",
        "\n",
        "    input_dim = t_x_train.shape[1]  # Количество признаков\\\n",
        "    transform = nn.Sequential(\n",
        "        nn.Linear(in_features=input_dim, out_features=6),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=6, out_features=1)\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    return transform\n",
        "\n",
        "model = build_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXezt60B4RDg"
      },
      "source": [
        "### Задание №12\n",
        "Приложите в ЛМС метрику качества этой сети после 2к итераций обучения.\n",
        "Эту модель можно обучить на CPU, не обязательно на видеокарте.\n",
        "\n",
        "Используйте для обучения ту же функцию `train_loop` с теми же параметрами (`learning rate`, число итераций и т.п.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGjK2-Jh4RDg"
      },
      "outputs": [],
      "source": [
        "train_loop(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Считаем финальную метрику на тестовых данных\n",
        "with torch.no_grad():\n",
        "    # Помним, что все тензоры должны быть на одном устройстве\n",
        "    out = model(t_x_test)\n",
        "    mse_loss = F.mse_loss(out, t_y_test).item()\n",
        "    print(\"Финальный MSE-лосс:\", mse_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dugFrug4RDg"
      },
      "source": [
        "### Задание №13\n",
        "Добавьте дополнительные слои в нейронную сеть\n",
        "\n",
        "Вам необходимо усложнить нейронную сеть, добавив еще 2-3 скрытых слоя с такими же размерностями, как в предыдущем задании.\n",
        "\n",
        "Приложите в лмс код, который в переменную `model` запишет вашу модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoE1-gBx4RDh"
      },
      "outputs": [],
      "source": [
        "def build_model(): ...\n",
        "\n",
        "\n",
        "model = build_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKvHs-Wg4RDh"
      },
      "source": [
        "### Задание №14\n",
        "Приложите в ЛМС метрику качества после 2к итераций обучения.\n",
        "\n",
        "Используйте для обучения ту же функцию `train_loop` с теми же параметрами (`learning rate`, число итераций и т.п.)\n",
        "\n",
        "Эту модель можно обучить на CPU, не обязательно на видеокарте."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIHuW0EO4RDh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPW4tENg4RDh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

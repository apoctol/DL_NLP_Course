{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Урок 4\n",
    "Эта демонстрация разбита на 3 ноутбука:\n",
    "\n",
    "1. Свертки и пулинги.\n",
    "2. **Даталоадеры.**\n",
    "3. Задача классификации с использованием CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Про загрузку данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во всех предыдущих примерах мы всегда учили сеть на всех данных сразу.\n",
    "Но это возможно не всегда.\n",
    "\n",
    "Хороший пример - [IMDB-WIKI](https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/), урезанная версия которого весит порядка 8 Gb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если работаете в colab, запустите команды ниже.\n",
    "# Они скачают и распакуют датасет.\n",
    "# Должна получиться папка imdb_crop.tar\n",
    "\n",
    "# !wget https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/static/imdb_crop.tar\n",
    "# !tar xf imdb_crop.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genders data: [1. 1. 1. ... 0. 0. 0.]\n",
      "path to imgs: imdb_crop/01/nm0000001_rm124825600_1899-5-10_1968.jpg\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "\n",
    "imdb_dat = loadmat(\"imdb_crop/imdb.mat\")[\"imdb\"][0][0]\n",
    "imdb_paths = [f\"imdb_crop/{path[0]}\" for path in imdb_dat[2][0]]\n",
    "imdb_genders = imdb_dat[3][0]\n",
    "# 1 означает Male, 0 - Female\n",
    "print(\"genders data:\", imdb_genders)\n",
    "print(\"path to imgs:\", imdb_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/460723 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 460723/460723 [00:00<00:00, 821986.56it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import tqdm\n",
    "\n",
    "total_size = 0\n",
    "for one_path in tqdm.tqdm(imdb_paths):\n",
    "    total_size += os.path.getsize(one_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.179882742464542"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# В гигабайтах\n",
    "total_size / 2**10 / 2**10 / 2**10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Учтите, что во время обучения нам нужно примерно х2 памяти - на прямой и обратный проход.\n",
    "\n",
    "Если учить все одним батчом, то будет 12+ Гб на видеокарте - такое уже не каждая GPU потянет.\n",
    "\n",
    "Без батчей тут не обойтись. Пойдем таким путем:\n",
    "\n",
    "- научимся загружать одну картинку в тензор;\n",
    "- научимся объединять несколько картинок в батчи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как загрузить одну картинку в тензор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(257, 257, 3)\n",
      "[[[ 12  17  13]\n",
      "  [ 11  16  12]\n",
      "  [ 13  15  12]\n",
      "  ...\n",
      "  [  8   9  11]\n",
      "  [  8   9  11]\n",
      "  [  9   9   9]]\n",
      "\n",
      " [[ 11  16  12]\n",
      "  [ 12  17  13]\n",
      "  [ 14  16  13]\n",
      "  ...\n",
      "  [  8   9  11]\n",
      "  [  8   9  11]\n",
      "  [  9   9   9]]\n",
      "\n",
      " [[ 13  15  12]\n",
      "  [ 14  16  13]\n",
      "  [ 16  18  15]\n",
      "  ...\n",
      "  [  8   9  11]\n",
      "  [  8   9  11]\n",
      "  [  9   9   9]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 50  47   4]\n",
      "  [ 50  47   6]\n",
      "  [ 54  50  12]\n",
      "  ...\n",
      "  [124  58  24]\n",
      "  [109  46  13]\n",
      "  [102  43  11]]\n",
      "\n",
      " [[ 56  53  10]\n",
      "  [ 55  52  11]\n",
      "  [ 56  52  14]\n",
      "  ...\n",
      "  [124  58  26]\n",
      "  [115  52  19]\n",
      "  [107  50  20]]\n",
      "\n",
      " [[ 63  59  14]\n",
      "  [ 60  55  13]\n",
      "  [ 57  52  14]\n",
      "  ...\n",
      "  [121  56  26]\n",
      "  [120  57  26]\n",
      "  [107  50  20]]]\n"
     ]
    }
   ],
   "source": [
    "# Вариант 1 - использовать matplotlib\n",
    "# С ним уже виделись ранее, когда работали с NotMNIST\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = plt.imread(imdb_paths[0])\n",
    "print(image.shape)\n",
    "print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "(257, 257, 3)\n",
      "[[[ 12  17  13]\n",
      "  [ 11  16  12]\n",
      "  [ 13  15  12]\n",
      "  ...\n",
      "  [  8   9  11]\n",
      "  [  8   9  11]\n",
      "  [  9   9   9]]\n",
      "\n",
      " [[ 11  16  12]\n",
      "  [ 12  17  13]\n",
      "  [ 14  16  13]\n",
      "  ...\n",
      "  [  8   9  11]\n",
      "  [  8   9  11]\n",
      "  [  9   9   9]]\n",
      "\n",
      " [[ 13  15  12]\n",
      "  [ 14  16  13]\n",
      "  [ 16  18  15]\n",
      "  ...\n",
      "  [  8   9  11]\n",
      "  [  8   9  11]\n",
      "  [  9   9   9]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 50  47   4]\n",
      "  [ 50  47   6]\n",
      "  [ 54  50  12]\n",
      "  ...\n",
      "  [124  58  24]\n",
      "  [109  46  13]\n",
      "  [102  43  11]]\n",
      "\n",
      " [[ 56  53  10]\n",
      "  [ 55  52  11]\n",
      "  [ 56  52  14]\n",
      "  ...\n",
      "  [124  58  26]\n",
      "  [115  52  19]\n",
      "  [107  50  20]]\n",
      "\n",
      " [[ 63  59  14]\n",
      "  [ 60  55  13]\n",
      "  [ 57  52  14]\n",
      "  ...\n",
      "  [121  56  26]\n",
      "  [120  57  26]\n",
      "  [107  50  20]]]\n"
     ]
    }
   ],
   "source": [
    "# Вариант 2 - использовать PIL\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Image.open вернет специальный объект Image\n",
    "image = Image.open(imdb_paths[0])\n",
    "print(type(image))\n",
    "# который легко конвертируется в numpy массив\n",
    "img_array = np.array(image)\n",
    "print(img_array.shape)\n",
    "print(img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(257, 257, 3)\n",
      "[[[ 13  17  12]\n",
      "  [ 12  16  11]\n",
      "  [ 12  15  13]\n",
      "  ...\n",
      "  [ 11   9   8]\n",
      "  [ 11   9   8]\n",
      "  [  9   9   9]]\n",
      "\n",
      " [[ 12  16  11]\n",
      "  [ 13  17  12]\n",
      "  [ 13  16  14]\n",
      "  ...\n",
      "  [ 11   9   8]\n",
      "  [ 11   9   8]\n",
      "  [  9   9   9]]\n",
      "\n",
      " [[ 12  15  13]\n",
      "  [ 13  16  14]\n",
      "  [ 15  18  16]\n",
      "  ...\n",
      "  [ 11   9   8]\n",
      "  [ 11   9   8]\n",
      "  [  9   9   9]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[  4  47  50]\n",
      "  [  6  47  50]\n",
      "  [ 12  50  54]\n",
      "  ...\n",
      "  [ 24  58 124]\n",
      "  [ 13  46 109]\n",
      "  [ 11  43 102]]\n",
      "\n",
      " [[ 10  53  56]\n",
      "  [ 11  52  55]\n",
      "  [ 14  52  56]\n",
      "  ...\n",
      "  [ 26  58 124]\n",
      "  [ 19  52 115]\n",
      "  [ 20  50 107]]\n",
      "\n",
      " [[ 14  59  63]\n",
      "  [ 13  55  60]\n",
      "  [ 14  52  57]\n",
      "  ...\n",
      "  [ 26  56 121]\n",
      "  [ 26  57 120]\n",
      "  [ 20  50 107]]]\n"
     ]
    }
   ],
   "source": [
    "# Вариант 3 - cv2 (a.k.a. opencv-python)\n",
    "import cv2\n",
    "\n",
    "cv_image = cv2.imread(imdb_paths[0])\n",
    "print(type(cv_image))\n",
    "print(cv_image.shape)\n",
    "print(cv_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основные различия: opencv и PIL имеют более богатый набор для редактирования самого изображения, но эти две библиотеки нужно отдельно установить.\n",
    "\n",
    "У opencv есть интеграция с `albumentations`, которую мы будем использовать, поэтому возьмем `cv2.imread`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как объединить несколько картинок в батч\n",
    "В PyTorch уже есть готовое решение того, как бить данные на батчи.\n",
    "Для этих целей используется **Dataset** и **DataLoader**.\n",
    "\n",
    "Но перед тем, как их использовать, нужно подчистить данные.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8462"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Первая проблема - в датасете не везде есть метки\n",
    "np.count_nonzero(np.isnan(imdb_genders))\n",
    "# Выкинем их"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_indices = set(np.where(np.isnan(imdb_genders))[0])\n",
    "imdb_paths = [x for i, x in enumerate(imdb_paths) if i not in bad_indices]\n",
    "imdb_genders = [int(x) for i, x in enumerate(imdb_genders) if i not in bad_indices]\n",
    "assert len(imdb_paths) == len(imdb_genders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(257, 257, 3)\n",
      "(263, 263, 3)\n"
     ]
    }
   ],
   "source": [
    "# Вторая проблема - картинки имеют разный размер\n",
    "print(cv2.imread(imdb_paths[0]).shape)\n",
    "print(cv2.imread(imdb_paths[1]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это плохо - нейросеть, которую мы дальше будем учить, не сумеет состыковать размерности.\n",
    "Помимо этого, мы не сможем собрать батч - у тензора жестко фиксирована размерность каждого среза.\n",
    "\n",
    "Поэтому придется привести все картинки к одному размеру!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 128, 3)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Есть библиотека albumentations, в которой есть часто используемые операции над картинками.\n",
    "# В частности, resize до фиксированной размерности\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "# Compose означает \"примени все трансформации из списка\"\n",
    "# У нас трансформация одна, но в будущем их может стать больше\n",
    "transforms = A.Compose([A.Resize(128, 128)])\n",
    "\n",
    "# В albumentations аргументы надо передавать с именем, на выходе будет словарь.\n",
    "# Передали по имени `image`, заберем тому же ключу.\n",
    "result = transforms(image=plt.imread(imdb_paths[0]))[\"image\"]\n",
    "print(result.shape)\n",
    "print(type(result))\n",
    "# Получили нужную размерность"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные почищены, идем разбивать на батчи.\n",
    "\n",
    "**Dataset** - это класс для хранения данных.\n",
    "Задача Dataset - уметь отдавать пользователю один элемент данных.\n",
    "Для этого нужно определить методы `__getitem__` и `__len__`.\n",
    "\n",
    "**DataLoader** - это класс, который умеет разрезать _Dataset_ на батчи.\n",
    "Он умеет бить на батчи, перемешивать их и загружать батчи параллельно с процессом обучения.\n",
    "\n",
    "Чтобы пользоваться _DataLoader_, нужно сначала обернуть данные в _Dataset_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # В методе __init__ можете сделать что угодно.\n",
    "        # Обычно здесь готовят переменные, которые помогут загрузить данные\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # __getitem__ должен отдать то, что вы считаете одним элементом датасета.\n",
    "        # Тип данных не ограничен.\n",
    "        # В переменной index лежит номер элемента, который заказал пользователь.\n",
    "        return 1\n",
    "\n",
    "    def __len__(self):\n",
    "        # __len__ должен вернуть количество элементов в датасете.\n",
    "        # Это должно быть целым числом.\n",
    "        return 1\n",
    "\n",
    "\n",
    "simple_dataset = SimpleDataset()\n",
    "print(len(simple_dataset))\n",
    "print(simple_dataset[0])\n",
    "print(simple_dataset[1])\n",
    "print(simple_dataset[100500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перейдем к нашему IMDB Wiki и попробуем написать для него Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 128, 128])\n",
      "1\n",
      "torch.Size([3, 128, 128])\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import albumentations as A\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from scipy.io import loadmat\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class ImdbWikiDataset(Dataset):\n",
    "    def __init__(self, image_size: int = 128):\n",
    "        # Из кодов выше\n",
    "        imdb_dat = loadmat(\"imdb_crop/imdb.mat\")[\"imdb\"][0][0]\n",
    "        imdb_paths = [f\"imdb_crop/{path[0]}\" for path in imdb_dat[2][0]]\n",
    "        imdb_genders = imdb_dat[3][0]\n",
    "        bad_indices = set(np.where(np.isnan(imdb_genders))[0])\n",
    "        imdb_paths = [x for i, x in enumerate(imdb_paths) if i not in bad_indices]\n",
    "        imdb_genders = [\n",
    "            int(x) for i, x in enumerate(imdb_genders) if i not in bad_indices\n",
    "        ]\n",
    "\n",
    "        # Не будем читать картинки при создании датасета, чтобы сберечь ОЗУ.\n",
    "        self.paths = imdb_paths\n",
    "        self.labels = imdb_genders\n",
    "        self.transforms = A.Compose(\n",
    "            [\n",
    "                # Подгонит под размер (128, 128)\n",
    "                A.Resize(image_size, image_size),\n",
    "                # A.HorizontalFlip(p=0.5),\n",
    "                # Пиксели в отрезке [0; 255] - это uint8.\n",
    "                # Переведем в отрезок [0.0; 1.0] - нейросети будет проще.\n",
    "                A.ToFloat(max_value=255),\n",
    "                # Поменяет (H, W, C) -> (C, H, W) и превратит в тензор PyTorch\n",
    "                ToTensorV2(),\n",
    "                # Для обогащения: будем переворачивать\n",
    "            ]\n",
    "        )\n",
    "        assert len(self.paths) == len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index) -> tuple[torch.Tensor, int]:\n",
    "        # Читать будем только одну картинку - и возвращать пару (тензор картинки, ее label)\n",
    "        img_numpy = cv2.imread(self.paths[index])\n",
    "        img_tensor = self.transforms(image=img_numpy)[\"image\"]\n",
    "\n",
    "        label = self.labels[index]\n",
    "        return img_tensor, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "\n",
    "dataset = ImdbWikiDataset()\n",
    "# Распечатаем несколько элементов из датасета\n",
    "# Выдаст пару (изображение, лейбл)\n",
    "one_item = dataset[0]\n",
    "print(one_item[0].shape)\n",
    "print(one_item[1])\n",
    "one_item = dataset[5]\n",
    "print(one_item[0].shape)\n",
    "print(one_item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(361809, 45226, 45226)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Разобьем на train/val/test\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Для воспроизводимости создадим генератор случайности\n",
    "# и зафиксируем ему seed.\n",
    "seed = 0\n",
    "generator = torch.Generator()\n",
    "generator.manual_seed(seed)\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [0.8, 0.1, 0.1], generator=generator\n",
    ")\n",
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Обернем датасет в DataLoader, передав batch_size\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    # перемешать данные или нет\n",
    "    shuffle=True,\n",
    "    # если перемешать - озаботьтесь воспроизводимостью\n",
    "    generator=generator,\n",
    "    # В последнем батче может не набраться 32 элемента.\n",
    "    # Этот флаг говорит, убрать такой батч или оставить.\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 128, 128])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# После этого можно итерироваться по DataLoader\n",
    "# Одна итерация = один батч\n",
    "for one_batch in train_loader:\n",
    "    batch_of_images, batch_of_labels = one_batch\n",
    "    print(type(batch_of_images))\n",
    "    print(batch_of_images.shape)\n",
    "    print(type(batch_of_labels))\n",
    "    print(batch_of_labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание: датасет возвращал тензор размера (3, 128, 128) и одно число, а вот DataLoader уже возвращет `(batch_size, 4, 128, 128)` и вектор из лейблов размера 32.\n",
    "\n",
    "Pytorch собрал все за нас в батч и состыковал объекты из датасета."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "start-dl-MwYW62ZD-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
